{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U0oHOMhk67k"
   },
   "source": [
    "# Môn: Máy học nâng cao\n",
    "* Sinh viên: Huỳnh Thị Bảo Trân\n",
    "* MSSV: 19110482\n",
    "* Bài thực hành: lab04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DhJ--5jrtuTn"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SpjwiLYuXnT",
    "outputId": "aec43982-b498-4d50-acdc-5f432ca2390a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.3)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tNxt4DQsTODz",
    "outputId": "4e56934b-079d-416e-91ad-3fccb2e90dd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Label  y\n",
       "0     Go until jurong point, crazy.. Available only ...   ham  0\n",
       "1                         Ok lar... Joking wif u oni...   ham  0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...  spam  1\n",
       "3     U dun say so early hor... U c already then say...   ham  0\n",
       "4     Nah I don't think he goes to usf, he lives aro...   ham  0\n",
       "...                                                 ...   ... ..\n",
       "5569  This is the 2nd time we have tried 2 contact u...  spam  1\n",
       "5570              Will Ã¼ b going to esplanade fr home?   ham  0\n",
       "5571  Pity, * was in mood for that. So...any other s...   ham  0\n",
       "5572  The guy did some bitching but I acted like i'd...   ham  0\n",
       "5573                         Rofl. Its true to its name   ham  0\n",
       "\n",
       "[5574 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv(\"spam_detection.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05ufiYEyTOD8",
    "outputId": "f2e6893b-91c4-4e46-d9c5-7a4640280d38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baotr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "texts = df[\"Text\"].to_list()\n",
    "texts = [text.lower() for text in texts]           # chuyển các đoạn text thành chữ thường (word embedding chỉ cho chữ thường)\n",
    "tokenized_texts = [nltk.tokenize.word_tokenize(text) for text in texts]    # tách câu thành một list các từ\n",
    "\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zxdndbsruel4",
    "outputId": "23a3c2a3-7b9c-473f-d9a2-9d5fb68d1e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-20 08:16:06--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-11-20 08:16:06--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-11-20 08:16:07--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 40s  \n",
      "\n",
      "2022-11-20 08:18:48 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa-aIBqVuvVj",
    "outputId": "487efbd8-e9f6-4944-df4f-fa19ce9795c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eamFERdDTOEC"
   },
   "outputs": [],
   "source": [
    "## không cần hiểu đống này lắm đâu\n",
    "import io\n",
    "import numpy as np\n",
    "def load_word_embeddings(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab, matrix = [], []\n",
    "    i=0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        vocab.append(tokens[0])\n",
    "        matrix.append(list(map(float, tokens[1:])))\n",
    "    return vocab, np.asarray(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LFpgCiITTOEG"
   },
   "outputs": [],
   "source": [
    "vocab, matrix = load_word_embeddings(\"glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3ZMmUs77TOEN"
   },
   "outputs": [],
   "source": [
    "## Gán các mã\n",
    "__PADDED_INDEX__ = 0    # mã dùng cho các vị trí chỉ có tính nối dài cho cùng kích thước\n",
    "__UNKNOWN_WORD__ = 1    # mã cho những từ không có trong embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EpIOHPXVTOER"
   },
   "outputs": [],
   "source": [
    "# Tạo một dictionary, có nhiệm vụ là một ánh xạ từ ảnh sang mã số, mã số được bắt đầu từ 2 vì số 0 và 1 được dành cho trường hợp đặc biệt\n",
    "word_to_index = {word: index+2 for index, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-5Da0MqTOEV",
    "outputId": "bbfcccec-be9c-4fc1-814b-b63614ef2a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [-0.038194 -0.24487   0.72812  ... -0.1459    0.8278    0.27062 ]\n",
      " ...\n",
      " [ 0.36088  -0.16919  -0.32704  ...  0.27139  -0.29188   0.16109 ]\n",
      " [-0.10461  -0.5047   -0.49331  ...  0.42527  -0.5125   -0.17054 ]\n",
      " [ 0.28365  -0.6263   -0.44351  ...  0.43678  -0.82607  -0.15701 ]]\n"
     ]
    }
   ],
   "source": [
    "# Do do mã số được bắt đầu từ 2, nên cần thêm 2 vector vào đàu ma trận\n",
    "embedding_matrix = np.pad(matrix, [[2,0],[0,0]], mode='constant', constant_values =0.0)\n",
    "print(embedding_matrix)\n",
    "\n",
    "# Khi đó, __PADDED_INDEX__ dùng dòng đầu tiên của  embedding_matrix\n",
    "# __UNKNOWN_WORD__ dùng dòng thứ hai của embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qtfhG_fWTOEa"
   },
   "outputs": [],
   "source": [
    "## Bây giờ ta sẽ chuyển data spam dection thành các mã số\n",
    "import tensorflow as tf\n",
    "\n",
    "X = []\n",
    "for text in tokenized_texts:\n",
    "    cur_text_indices = []\n",
    "    for word in text:\n",
    "        if word in word_to_index:\n",
    "            cur_text_indices.append(word_to_index[word])    ## map từ word sang index\n",
    "        else:\n",
    "            cur_text_indices.append(__UNKNOWN_WORD__)       ## gán unknown cho từ không có trong bộ glove\n",
    "    X.append(cur_text_indices)\n",
    "\n",
    "## pad data cho có cùng chiều dài\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences=X,       # sequences: list các câu có độ dài không bằng nhau\n",
    "                                                  padding='post')    # vị trí pad là 'pre' (trước) hoặc 'post' (sau)\n",
    "\n",
    "y = df['y'].values   ## Label của bài toán, 0 là không phải spam, 1 là spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uMcpmdJBTOEe"
   },
   "outputs": [],
   "source": [
    "## Chia data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEliDcAPw5cs"
   },
   "source": [
    "## Mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLgUeSE-w4_i",
    "outputId": "35c48818-dea5-45a2-fed5-0ed4b1d53a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         40000200  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,080,802\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 40,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(None,))                   ## None biểu thị kích thước không xác định của câu\n",
    "\n",
    "embed = Embedding(input_dim=embedding_matrix.shape[0],   ## Khai báo kích thước của vocab\n",
    "                 output_dim=embedding_matrix.shape[1],   ## Khai báo kích thước của embedding\n",
    "                  embeddings_initializer = tf.keras.initializers.Constant(value=embedding_matrix),  ## Khởi tạo cho embedding bằng ma trận có sẵn\n",
    "                  trainable=False,                       ## Không cần thiết train embedding\n",
    "                 mask_zero=True)(inputs)                 ## zero_mask: những vị trí có giá trị 0 không được tính toán, vì đó là giá trị thêm vào cho đủ độ dài mà thôi\n",
    "                                                         ##  (__PADDED_INDEX__ gán bằng 0)\n",
    "\n",
    "lstm = LSTM(units=100,                          ## units: kích thước của hidden_state trong LSTM\n",
    "            return_sequences=False)(embed)      ## return_sequences: LSTM trả về toàn bộ  hay là trả về hidden_state cuối cùng\n",
    "\n",
    "dense = Dense(units=2, activation='softmax')(lstm)\n",
    "model = Model(inputs=inputs,\n",
    "              outputs=dense)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXsAo4Y_w_TN",
    "outputId": "dab804cf-bbe5-4d3a-db5e-a37057207634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 [==============================] - 52s 334ms/step - loss: 0.1615 - accuracy: 0.9415 - val_loss: 0.0978 - val_accuracy: 0.9722\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 42s 298ms/step - loss: 0.0796 - accuracy: 0.9726 - val_loss: 0.0753 - val_accuracy: 0.9776\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 41s 297ms/step - loss: 0.0577 - accuracy: 0.9827 - val_loss: 0.0741 - val_accuracy: 0.9758\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 42s 298ms/step - loss: 0.0510 - accuracy: 0.9839 - val_loss: 0.0639 - val_accuracy: 0.9803\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 18s 131ms/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 0.0652 - val_accuracy: 0.9785\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 19s 134ms/step - loss: 0.0334 - accuracy: 0.9899 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 19s 136ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.0696 - val_accuracy: 0.9803\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 19s 139ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.0853 - val_accuracy: 0.9713\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 19s 135ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0669 - val_accuracy: 0.9830\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 19s 136ms/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 0.0659 - val_accuracy: 0.9812\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0639 - accuracy: 0.9803\n",
      "Accuracy on valid:  0.9802690744400024\n"
     ]
    }
   ],
   "source": [
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath=\"lstm_spam.h5\", \n",
    "                                     monitor='val_loss',\n",
    "                                     mode='min', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=True)\n",
    "# Train\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "         epochs=10,\n",
    "         callbacks=[mc])\n",
    "\n",
    "model.load_weights(\"lstm_spam.h5\")\n",
    "_, val_acc = model.evaluate(X_valid, y_valid)\n",
    "print(\"Accuracy on valid: \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbeXf-8bw2No"
   },
   "source": [
    "# Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjdAFWRLx46W"
   },
   "source": [
    "Hãy train lại model sao cho embedding layer có tính learnable Kèm theo đó là dùng bộ glove 6B 300d\n",
    "\n",
    "Hãy viết một hàm\n",
    "\n",
    "def model_predict(text):\n",
    "\n",
    "  return True/False\n",
    "\n",
    "\n",
    "Tự điều chỉnh và train model (chỉnh lại train, valid, tiền xử lý, dùng word_embedding,... nếu muốn) rồi đoán xem các câu sau có phải spam không:\n",
    "\"wanna ask something? just send me a mess\"\n",
    "\n",
    "\"Urgent! You have won our competition's prize!! Please call us now.\"\n",
    "\n",
    "\"Call me to get a free holiday now\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjFsFmqkjE_D"
   },
   "source": [
    "# Bài làm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yENinMy6hogo"
   },
   "outputs": [],
   "source": [
    "vocab, matrix = load_word_embeddings('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NDBdhSEOhog3"
   },
   "outputs": [],
   "source": [
    "word_to_index = {word: index + 2 for index, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qM_x5vVohog3",
    "outputId": "85bd55a9-669f-4e98-fc20-295b0b62100a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...  0.0090611 -0.20989    0.053913 ]\n",
      " ...\n",
      " [ 0.075713  -0.040502   0.18345   ...  0.21838    0.30967    0.43761  ]\n",
      " [ 0.81451   -0.36221    0.31186   ...  0.075486   0.28408   -0.17559  ]\n",
      " [ 0.429191  -0.296897   0.15011   ...  0.28975    0.32618   -0.0590532]]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.pad(matrix, [[2, 0], [0, 0]], mode = 'constant', constant_values = 0.0)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U6_n3Ka5hog3"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for text in tokenized_texts:\n",
    "    cur_text_indices = []\n",
    "    for word in text:\n",
    "        if word in word_to_index:\n",
    "            cur_text_indices.append(word_to_index[word])\n",
    "        else:\n",
    "            cur_text_indices.append(__UNKNOWN_WORD__)\n",
    "    X.append(cur_text_indices)\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences = X, padding = 'post')\n",
    "\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Dr_1OScThog4"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VBn3EFhog4"
   },
   "source": [
    "## Mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4FnLwDehog4",
    "outputId": "b02f9561-c245-4120-b9a2-0457411d1d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 300)         120000600 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,161,202\n",
      "Trainable params: 160,602\n",
      "Non-trainable params: 120,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (None, ))\n",
    "\n",
    "embed = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1],\n",
    "                  embeddings_initializer = tf.keras.initializers.Constant(value = embedding_matrix),\n",
    "                  trainable = False,\n",
    "                  mask_zero = True)(inputs)\n",
    "\n",
    "lstm = LSTM(units = 100, return_sequences = False)(embed)      \n",
    "\n",
    "dense = Dense(units = 2, activation = 'softmax')(lstm)\n",
    "model = Model(inputs = inputs, outputs = dense)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIpq0r01h-7p",
    "outputId": "1a8f9971-9f70-4153-95d2-631beebca5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36/36 [==============================] - 223s 6s/step - loss: 0.2389 - accuracy: 0.9101 - val_loss: 0.1013 - val_accuracy: 0.9695\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 246s 7s/step - loss: 0.0768 - accuracy: 0.9758 - val_loss: 0.0889 - val_accuracy: 0.9677\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.0677 - accuracy: 0.9776 - val_loss: 0.0682 - val_accuracy: 0.9776\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 274s 8s/step - loss: 0.0443 - accuracy: 0.9865 - val_loss: 0.0573 - val_accuracy: 0.9821\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 33s 837ms/step - loss: 0.0377 - accuracy: 0.9888 - val_loss: 0.0592 - val_accuracy: 0.9803\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 249s 7s/step - loss: 0.0338 - accuracy: 0.9899 - val_loss: 0.0529 - val_accuracy: 0.9857\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.0217 - accuracy: 0.9944 - val_loss: 0.0449 - val_accuracy: 0.9910\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 264s 7s/step - loss: 0.0182 - accuracy: 0.9960 - val_loss: 0.0429 - val_accuracy: 0.9883\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 36s 964ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.0607 - val_accuracy: 0.9857\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 29s 803ms/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.0588 - val_accuracy: 0.9830\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 28s 779ms/step - loss: 0.0123 - accuracy: 0.9973 - val_loss: 0.0584 - val_accuracy: 0.9857\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 27s 751ms/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 0.0500 - val_accuracy: 0.9883\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 27s 745ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0531 - val_accuracy: 0.9865\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 26s 720ms/step - loss: 0.0033 - accuracy: 0.9996 - val_loss: 0.0596 - val_accuracy: 0.9874\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 28s 769ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0649 - val_accuracy: 0.9839\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 26s 717ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0614 - val_accuracy: 0.9874\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 26s 712ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0554 - val_accuracy: 0.9892\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 26s 716ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0583 - val_accuracy: 0.9874\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 26s 717ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0588 - val_accuracy: 0.9874\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 25s 707ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0621 - val_accuracy: 0.9883\n",
      "35/35 [==============================] - 3s 72ms/step - loss: 0.0429 - accuracy: 0.9883\n",
      "Accuracy on valid:  0.9883407950401306\n"
     ]
    }
   ],
   "source": [
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath = 'lstm_spam.h5', monitor = 'val_loss', mode = 'min', verbose = 0, save_best_only = True)\n",
    "# Train\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 20, batch_size = 124, validation_split = 0.15, callbacks = [mc])\n",
    "\n",
    "model.load_weights('lstm_spam.h5')\n",
    "_, val_acc = model.evaluate(X_valid, y_valid)\n",
    "print('Accuracy on valid: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d8LWrHQ0G4-l",
    "outputId": "38d3e892-5f9a-41b9-8598-65b58498daa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "urgent! you have won our competition's prize!! please call us now.\n",
      "False\n",
      "call me to get a free holiday now\n",
      "True\n",
      "fair enough, anything going on?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "texts = (\"Urgent! You have won our competition's prize!! Please call us now.\", \n",
    "         \"Call me to get a free holiday now\", \n",
    "         \"Fair enough, anything going on?\")\n",
    "texts = [text.lower() for text in texts]\n",
    "tokenized_texts = [nltk.tokenize.word_tokenize(text) for text in texts]\n",
    "pad = []\n",
    "for vocab in tokenized_texts:\n",
    "    cur_text_indices = []\n",
    "    for word in vocab:\n",
    "        if word in word_to_index:\n",
    "            cur_text_indices.append(word_to_index[word])\n",
    "        else:\n",
    "            cur_text_indices.append(__UNKNOWN_WORD__)\n",
    "    pad.append(cur_text_indices)\n",
    "\n",
    "pad = tf.keras.preprocessing.sequence.pad_sequences(sequences = pad, padding = 'post')\n",
    "pre = model.predict(pad)\n",
    "for i in range(len(texts)):\n",
    "    print(texts[i] + '\\n' + str(np.round(pre[i][0]) > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IxNXGOMBuKjf",
    "outputId": "49b52a58-a99e-4bf7-e786-f7e7430ba730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "urgent! you have won our competition's prize!! please call us now.\n",
      "False\n",
      "call me to get a free holiday now\n",
      "True\n",
      "fair enough, anything going on?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def model_predict(texts):\n",
    "    texts = [text.lower() for text in texts]\n",
    "    tokenized_texts = [nltk.tokenize.word_tokenize(text) for text in texts]\n",
    "    pad = []\n",
    "    for vocab in tokenized_texts:\n",
    "        cur_text_indices = []\n",
    "        for word in vocab:\n",
    "            if word in word_to_index:\n",
    "                cur_text_indices.append(word_to_index[word])\n",
    "            else:\n",
    "                cur_text_indices.append(__UNKNOWN_WORD__)\n",
    "        pad.append(cur_text_indices)\n",
    "\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences(sequences = pad, padding = 'post')\n",
    "    pre = model.predict(pad)\n",
    "    for i in range(len(texts)):\n",
    "        print(texts[i] + '\\n' + str(np.round(pre[i][0]) > 0.5))\n",
    "\n",
    "texts = (\"Urgent! You have won our competition's prize!! Please call us now.\", \n",
    "         \"Call me to get a free holiday now\", \n",
    "         \"Fair enough, anything going on?\")\n",
    "model_predict(texts)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
